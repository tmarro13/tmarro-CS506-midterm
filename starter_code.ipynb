{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-b4096f28f6b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Load the training data in chunks if it's too large\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_large_dataset(filepath, chunksize=100000):\n",
    "    print(\"Loading dataset in chunks...\")\n",
    "    chunks = []\n",
    "    for chunk in tqdm(pd.read_csv(filepath, chunksize=chunksize), desc=\"Loading chunks\"):\n",
    "        chunks.append(chunk)\n",
    "    return pd.concat(chunks, axis=0)\n",
    "# Preprocessing and feature engineering\n",
    "def preprocess_data(df, is_training=True):\n",
    "    print(f\"Preprocessing data (Training: {is_training})...\")\n",
    "    # Example preprocessing: Handle missing values\n",
    "    if is_training:\n",
    "        df = df.dropna(subset=['Score'])  # Remove rows where target is missing\n",
    "\n",
    "    # Ensure that 'Score' is categorical (e.g., convert to integers if it's float)\n",
    "    if 'Score' in df.columns and is_training:\n",
    "        df['Score'] = df['Score'].astype(int)  # Converting 'Score' to an integer type\n",
    "\n",
    "    # Drop columns that should not be used for training\n",
    "    if 'Id' in df.columns:\n",
    "        df = df.drop(columns=['Id'])  # Dropping ID column\n",
    "\n",
    "    # Identify non-numeric columns that need encoding\n",
    "    non_numeric_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in non_numeric_cols:\n",
    "        if col != 'Score':  # Ensure we don't encode the target variable\n",
    "            # Use Label Encoding for categorical columns\n",
    "            label_enc = LabelEncoder()\n",
    "            df[col] = label_enc.fit_transform(df[col].astype(str))\n",
    "    \n",
    "    # Example text processing: Use Tfidf if there's a text column\n",
    "    if 'review_text' in df.columns:\n",
    "        print(\"Applying TF-IDF transformation...\")\n",
    "        tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "        if is_training:\n",
    "            text_features = tfidf.fit_transform(df['review_text']).toarray()\n",
    "        else:\n",
    "            text_features = tfidf.transform(df['review_text']).toarray()\n",
    "        df = pd.concat([df.drop(columns=['review_text']), pd.DataFrame(text_features)], axis=1)\n",
    "        print(\"TF-IDF transformation completed.\")\n",
    "\n",
    "    # Handling numerical columns\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    print(f\"Standardizing numerical columns: {list(numerical_cols)}\")\n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Feature engineering and splitting data\n",
    "def prepare_data(df):\n",
    "    print(\"Splitting data into training and testing sets...\")\n",
    "    X = df.drop(['Score'], axis=1)\n",
    "    y = df['Score']\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model training\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test):\n",
    "    print(\"Training Random Forest model...\")\n",
    "    # Using a Random Forest as an example\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions and evaluation\n",
    "    print(\"Model training completed. Evaluating...\")\n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Prediction and submission creation\n",
    "def create_submission(model, test_filepath):\n",
    "    print(\"Creating submission file...\")\n",
    "    # Load and preprocess the test dataset\n",
    "    test_df = pd.read_csv(test_filepath)\n",
    "    test_df_processed = preprocess_data(test_df, is_training=False)\n",
    "    \n",
    "    # Making predictions\n",
    "    X_submission = test_df_processed.drop(columns=['Score'], errors='ignore')\n",
    "    X_submission['Score'] = model.predict(X_submission)\n",
    "    \n",
    "    # Create the submission file\n",
    "    submission = X_submission[['Id', 'Score']]\n",
    "    submission.to_csv(\"./data/submission.csv\", index=False)\n",
    "    print(\"Submission file created at './data/submission.csv'\")\n",
    "\n",
    "# Main function to execute the pipeline\n",
    "def main():\n",
    "    train_filepath = './data/train.csv'  # Adjusted path to train file\n",
    "    test_filepath = './data/test.csv'    # Adjusted path to test file\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"Starting data loading...\")\n",
    "    df = load_large_dataset(train_filepath)\n",
    "    print(\"Data loading completed.\")\n",
    "    \n",
    "    # Preprocess the data\n",
    "    df = preprocess_data(df)\n",
    "    \n",
    "    # Split the data into training and testing\n",
    "    X_train, X_test, y_train, y_test = prepare_data(df)\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    model = train_and_evaluate(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Create submission\n",
    "    create_submission(model, test_filepath)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Call the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
