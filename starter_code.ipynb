{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data loading...\n",
      "Loading dataset in chunks...\n",
      "Loaded 100000 rows so far...\n",
      "Loaded 200000 rows so far...\n",
      "Loaded 300000 rows so far...\n",
      "Loaded 400000 rows so far...\n",
      "Loaded 500000 rows so far...\n",
      "Loaded 600000 rows so far...\n",
      "Loaded 700000 rows so far...\n",
      "Loaded 800000 rows so far...\n",
      "Loaded 900000 rows so far...\n",
      "Loaded 1000000 rows so far...\n",
      "Loaded 1100000 rows so far...\n",
      "Loaded 1200000 rows so far...\n",
      "Loaded 1300000 rows so far...\n",
      "Loaded 1400000 rows so far...\n",
      "Loaded 1500000 rows so far...\n",
      "Loaded 1600000 rows so far...\n",
      "Loaded 1700000 rows so far...\n",
      "Data loading completed.\n",
      "Preprocessing data (Training: True)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-130e9506d4f1>:157: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Score'] = df['Score'].astype(int)  # Converting 'Score' to an integer type\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing numerical columns: ['HelpfulnessNumerator', 'HelpfulnessDenominator', 'Time']\n",
      "Splitting data into training and testing sets...\n",
      "Training Random Forest model...\n",
      "Model training completed. Evaluating...\n",
      "Accuracy: 0.560829302283308\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.49      0.40      0.44     18074\n",
      "           2       0.26      0.07      0.11     17604\n",
      "           3       0.33      0.10      0.15     35179\n",
      "           4       0.37      0.13      0.19     67127\n",
      "           5       0.60      0.92      0.73    159085\n",
      "\n",
      "    accuracy                           0.56    297069\n",
      "   macro avg       0.41      0.32      0.32    297069\n",
      "weighted avg       0.49      0.56      0.48    297069\n",
      "\n",
      "Creating submission file...\n",
      "Test data loaded with shape: (212192, 2)\n",
      "Preprocessing data (Training: False)...\n",
      "Standardizing numerical columns: ['Score']\n",
      "Test data processed with shape: (212192, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marro\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py:847: RuntimeWarning: invalid value encountered in true_divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "c:\\Users\\marro\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py:689: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  result = op(x, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created at './data/submission.csv'\n",
      "Starting data loading...\n",
      "Loading dataset in chunks...\n",
      "Loaded 100000 rows so far...\n",
      "Loaded 200000 rows so far...\n",
      "Loaded 300000 rows so far...\n",
      "Loaded 400000 rows so far...\n",
      "Loaded 500000 rows so far...\n",
      "Loaded 600000 rows so far...\n",
      "Loaded 700000 rows so far...\n",
      "Loaded 800000 rows so far...\n",
      "Loaded 900000 rows so far...\n",
      "Loaded 1000000 rows so far...\n",
      "Loaded 1100000 rows so far...\n",
      "Loaded 1200000 rows so far...\n",
      "Loaded 1300000 rows so far...\n",
      "Loaded 1400000 rows so far...\n",
      "Loaded 1500000 rows so far...\n",
      "Loaded 1600000 rows so far...\n",
      "Loaded 1700000 rows so far...\n",
      "Data loading completed.\n",
      "Preprocessing data (Training: True)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-130e9506d4f1>:157: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Score'] = df['Score'].astype(int)  # Converting 'Score' to an integer type\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing numerical columns: ['HelpfulnessNumerator', 'HelpfulnessDenominator', 'Time']\n",
      "Splitting data into training and testing sets...\n",
      "Training Random Forest model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-130e9506d4f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[1;31m# Call the main function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-130e9506d4f1>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[1;31m# Train and evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m     \u001b[1;31m# Create submission\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-130e9506d4f1>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;31m# Using a Random Forest as an example\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;31m# Predictions and evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\marro\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[1;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;31m# since correctness does not rely on using threads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[0;32m    388\u001b[0m                              \u001b[1;33m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'threads'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m                 delayed(_parallel_build_trees)(\n",
      "\u001b[1;32mc:\\Users\\marro\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1044\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1045\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\marro\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\marro\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\marro\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\marro\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\marro\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\marro\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\marro\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\marro\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    167\u001b[0m                                                         indices=indices)\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\marro\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    896\u001b[0m         \"\"\"\n\u001b[0;32m    897\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 898\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m    899\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\marro\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    387\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from lightgbm import LGBMClassifier  # Use LightGBM for improved efficiency\n",
    "from tqdm import tqdm\n",
    "from textblob import TextBlob\n",
    "\n",
    "def load_large_dataset(filepath, chunksize=100000):\n",
    "    print(\"Loading dataset in chunks...\")\n",
    "    chunks = []\n",
    "    for chunk in tqdm(pd.read_csv(filepath, chunksize=chunksize), desc=\"Loading chunks\"):\n",
    "        chunks.append(chunk)\n",
    "    return pd.concat(chunks, axis=0)\n",
    "\n",
    "# Preprocessing and feature engineering\n",
    "def preprocess_data(df, is_training=True):\n",
    "    print(f\"Preprocessing data (Training: {is_training})...\")\n",
    "    \n",
    "    # Handle missing values in the target\n",
    "    if is_training:\n",
    "        df = df.dropna(subset=['Score'])  # Remove rows where target is missing\n",
    "    \n",
    "    # Convert 'Score' to integers\n",
    "    if 'Score' in df.columns and is_training:\n",
    "        df['Score'] = df['Score'].astype(int)\n",
    "    \n",
    "    # Drop columns that should not be used for training\n",
    "    if 'Id' in df.columns:\n",
    "        df = df.drop(columns=['Id'])  # Dropping ID column\n",
    "    \n",
    "    # Add new features: sentiment polarity, review length, and additional text features\n",
    "    if 'review_text' in df.columns:\n",
    "        print(\"Extracting new features from review text...\")\n",
    "        df['sentiment_polarity'] = df['review_text'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
    "        df['review_length'] = df['review_text'].apply(lambda x: len(str(x).split()))\n",
    "        df['special_chars_count'] = df['review_text'].apply(lambda x: sum(1 for char in str(x) if char in ['!', '?']))\n",
    "        df['avg_word_length'] = df['review_text'].apply(lambda x: np.mean([len(word) for word in str(x).split()]) if str(x).split() else 0)\n",
    "    \n",
    "    # Identify non-numeric columns that need encoding\n",
    "    non_numeric_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in non_numeric_cols:\n",
    "        if col != 'Score':  # Ensure we don't encode the target variable\n",
    "            label_enc = LabelEncoder()\n",
    "            df[col] = label_enc.fit_transform(df[col].astype(str))\n",
    "    \n",
    "    # Optimized Text Vectorization\n",
    "    if 'review_text' in df.columns:\n",
    "        print(\"Applying optimized text vectorization...\")\n",
    "        hashing = HashingVectorizer(n_features=3000, alternate_sign=False)  # Use HashingVectorizer for faster processing\n",
    "        text_features = hashing.transform(df['review_text'])\n",
    "        df = pd.concat([df.drop(columns=['review_text']), pd.DataFrame(text_features.toarray())], axis=1)\n",
    "        print(\"Text vectorization completed.\")\n",
    "    \n",
    "    # Handling numerical columns\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    print(f\"Standardizing numerical columns: {list(numerical_cols)}\")\n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Feature engineering and splitting data\n",
    "def prepare_data(df):\n",
    "    print(\"Splitting data into training and testing sets...\")\n",
    "    X = df.drop(['Score'], axis=1)\n",
    "    y = df['Score']\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model training with LightGBM\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test):\n",
    "    print(\"Training LightGBM model with adjusted class weights...\")\n",
    "    model = LGBMClassifier(\n",
    "        n_estimators=150,  # Limited to 150 trees for efficiency\n",
    "        max_depth=15,  # Limit depth for balanced performance\n",
    "        learning_rate=0.1,\n",
    "        class_weight='balanced',  # Adjust for class imbalance\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions and evaluation\n",
    "    print(\"Model training completed. Evaluating...\")\n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Prediction and submission creation\n",
    "def create_submission(model, test_filepath):\n",
    "    print(\"Creating submission file...\")\n",
    "    # Load and preprocess the test dataset\n",
    "    test_df = pd.read_csv(test_filepath)\n",
    "    test_df_processed = preprocess_data(test_df, is_training=False)\n",
    "    \n",
    "    # Making predictions\n",
    "    X_submission = test_df_processed.drop(columns=['Score'], errors='ignore')\n",
    "    X_submission['Score'] = model.predict(X_submission)\n",
    "    \n",
    "    # Create the submission file\n",
    "    submission = X_submission[['Id', 'Score']]\n",
    "    submission.to_csv(\"./data/submission.csv\", index=False)\n",
    "    print(\"Submission file created at './data/submission.csv'\")\n",
    "\n",
    "# Main function to execute the pipeline\n",
    "def main():\n",
    "    train_filepath = './data/train.csv'  # Adjusted path to train file\n",
    "    test_filepath = './data/test.csv'    # Adjusted path to test file\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"Starting data loading...\")\n",
    "    df = load_large_dataset(train_filepath)\n",
    "    print(\"Data loading completed.\")\n",
    "    \n",
    "    # Preprocess the data\n",
    "    df = preprocess_data(df)\n",
    "    \n",
    "    # Split the data into training and testing\n",
    "    X_train, X_test, y_train, y_test = prepare_data(df)\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    model = train_and_evaluate(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Create submission\n",
    "    create_submission(model, test_filepath)\n",
    "    \n",
    "    return model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the training data in chunks if it's too large\n",
    "def load_large_dataset(filepath, chunksize=100000):\n",
    "    print(\"Loading dataset in chunks...\")\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(filepath, chunksize=chunksize):\n",
    "        chunks.append(chunk)\n",
    "        print(f\"Loaded {len(chunks) * chunksize} rows so far...\")\n",
    "    return pd.concat(chunks, axis=0)\n",
    "\n",
    "# Preprocessing and feature engineering\n",
    "def preprocess_data(df, is_training=True):\n",
    "    print(f\"Preprocessing data (Training: {is_training})...\")\n",
    "    # Example preprocessing: Handle missing values\n",
    "    if is_training:\n",
    "        df = df.dropna(subset=['Score'])  # Remove rows where target is missing\n",
    "\n",
    "    # Ensure that 'Score' is categorical (e.g., convert to integers if it's float)\n",
    "    if 'Score' in df.columns and is_training:\n",
    "        df['Score'] = df['Score'].astype(int)  # Converting 'Score' to an integer type\n",
    "\n",
    "    # Drop columns that should not be used for training\n",
    "    if 'Id' in df.columns:\n",
    "        df = df.drop(columns=['Id'])  # Dropping ID column\n",
    "\n",
    "    # Identify non-numeric columns that need encoding\n",
    "    non_numeric_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in non_numeric_cols:\n",
    "        if col != 'Score':  # Ensure we don't encode the target variable\n",
    "            # Use Label Encoding for categorical columns\n",
    "            label_enc = LabelEncoder()\n",
    "            df[col] = label_enc.fit_transform(df[col].astype(str))\n",
    "    \n",
    "    # Example text processing: Use Tfidf if there's a text column\n",
    "    if 'review_text' in df.columns:\n",
    "        print(\"Applying TF-IDF transformation...\")\n",
    "        tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "        if is_training:\n",
    "            text_features = tfidf.fit_transform(df['review_text']).toarray()\n",
    "        else:\n",
    "            text_features = tfidf.transform(df['review_text']).toarray()\n",
    "        df = pd.concat([df.drop(columns=['review_text']), pd.DataFrame(text_features)], axis=1)\n",
    "        print(\"TF-IDF transformation completed.\")\n",
    "\n",
    "    # Handling numerical columns\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    print(f\"Standardizing numerical columns: {list(numerical_cols)}\")\n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Feature alignment function\n",
    "def align_features(X_train, X_submission):\n",
    "    # Get the training feature names\n",
    "    train_columns = X_train.columns\n",
    "    \n",
    "    # Add any missing columns in the submission set\n",
    "    for col in train_columns:\n",
    "        if col not in X_submission:\n",
    "            X_submission[col] = 0  # Fill missing features with 0\n",
    "    \n",
    "    # Ensure the same column order\n",
    "    X_submission = X_submission[train_columns]\n",
    "    \n",
    "    return X_submission\n",
    "\n",
    "# Feature engineering and splitting data\n",
    "def prepare_data(df):\n",
    "    print(\"Splitting data into training and testing sets...\")\n",
    "    X = df.drop(['Score'], axis=1)\n",
    "    y = df['Score']\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model training\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test):\n",
    "    print(\"Training Random Forest model...\")\n",
    "    # Using a Random Forest as an example\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions and evaluation\n",
    "    print(\"Model training completed. Evaluating...\")\n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Prediction and submission creation\n",
    "def create_submission(model, X_train, test_filepath):\n",
    "    print(\"Creating submission file...\")\n",
    "    \n",
    "    # Load and preprocess the test dataset\n",
    "    test_df = pd.read_csv(test_filepath)\n",
    "    print(f\"Test data loaded with shape: {test_df.shape}\")\n",
    "    \n",
    "    # Preprocess the test data\n",
    "    test_df_processed = preprocess_data(test_df, is_training=False)\n",
    "    print(f\"Test data processed with shape: {test_df_processed.shape}\")\n",
    "    \n",
    "    # Prepare the test data for prediction\n",
    "    X_submission = test_df_processed.drop(columns=['Score'], errors='ignore')\n",
    "    X_submission = align_features(X_train, X_submission)  # Align features with training data\n",
    "    \n",
    "    # Make predictions\n",
    "    X_submission['Score'] = model.predict(X_submission)\n",
    "    \n",
    "    # Create the submission file\n",
    "    submission = test_df[['Id']].copy()  # Ensuring the 'Id' column is intact\n",
    "    submission['Score'] = X_submission['Score']\n",
    "    submission.to_csv(\"./data/submission.csv\", index=False)\n",
    "    print(\"Submission file created at './data/submission.csv'\")\n",
    "\n",
    "# Main function to execute the pipeline\n",
    "def main():\n",
    "    train_filepath = './data/train.csv'  # Adjusted path to train file\n",
    "    test_filepath = './data/test.csv'    # Adjusted path to test file\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"Starting data loading...\")\n",
    "    df = load_large_dataset(train_filepath)\n",
    "    print(\"Data loading completed.\")\n",
    "    \n",
    "    # Preprocess the data\n",
    "    df = preprocess_data(df)\n",
    "    \n",
    "    # Split the data into training and testing\n",
    "    X_train, X_test, y_train, y_test = prepare_data(df)\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    model = train_and_evaluate(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Create submission\n",
    "    create_submission(model, X_train, test_filepath)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Call the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
